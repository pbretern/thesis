
\chapter{Schluss}
\label{chap:six}
%\section{Soll-Ist-Vergleich (Stand der Umsetzung)}
Mit der vorliegenden Arbeit sollte ein Dashboard als Proof-of-Concept entwickelt werden.
Das ist gelungen. Wie im Kapitel 5 dargelegt wurden fast alle Anforderungen und Anwendungsfälle umgesetzt.
Offen blieben die Anwendungsfälle 2 und 7, die aus zeitlichen Gründen nicht mehr bearbeitet werden konnten.
Neben diesen beiden Anwendungsfällen könnten auch andere Themen im Anschluss noch umgesetzt werden beziehungsweise
Verbesserungen am System oder im Erscheinungsbild des Dashboards vollzogen werden. Dazu gehört weitere Daten miteinzubeziehen
in die statistische Auswertung. Die Nutzung auf elektronische Ressourcen spielt innerhalb der MPG eine wichtige Rolle.
So könnte die Auswertung von \acrshort{COP 5}-Statistiken weiteren Aufschluss bieten, welche Ressourcen genutzt werden.
Ebenso könnte die Auswertung der Herkunft der zur Verfügung gestellten wissenschaftlichen Artikel für die Wissenschaftler:innen
Indizien geben, welche Zeitschriftenabonement eventuell an- oder abgeschafft werden könnten.
Im Bereich des Daten-Imports könnte man den Import automatisch über einen Cron-Job regeln, der einmal im Monat zum Beispiel die
Daten vollautomatisch importiert. Dabei wäre wichtig die Ergebnisse des Import-Prozesses in einer log-Datei zu protokollieren.
Umgesetzt werden könnte dies mit der Python Bibliothek Logging. Weiterhin könnte darüber nachgedacht werden, ob es
besser wäre, die Daten in eine Datenbank zu schreiben hinsichtlich effizienter Abfrage. Außerdem wäre zu überlegen, ob nicht die
unterschiedlichen Datensets zusammengeführt werden könnten im Sinne von den Business-Intelligence-Systemen und man dort eben
die Daten nach mehreren Dimensionen auswerten könnte. Ein Beispiel wären hier die Bestands- und Ausleihdaten. Die Zusammenführung
unterschiedlicher Datensets konnte in dieser Arbeit nicht geleistet werden.
Kleinere Verbesserungen wären die Auswertungen nach RVK-Fachsystematiken mit den Benennungen zu machen, da es die Aussagekraft der
Daten besser unterstreicht und für das Verständnis für ein fachfremdes Publikum verbessert.
Auch wäre denkbar die Daten nach anderen Anforderungen zu untersuchen und mit fortgeschritteneren statistischen Methoden auszuwerten.
So zum Beispiel wäre die Korrelation zwischen Bestand und Ausleihe interessant und diese innerhalb eines Diagramms zu zeigen.  
Auf der Seite der Erscheinung und der Darstellung im Dashboard müsste auch noch einiges getan werden. So zum Beispiel könnte das Dashboard-Layout
mit Responsive-Design an mobile Geräte angepasst werden. Dies könnte geschehen durch mehr Einsatz der dash\_bootstrap\_components.
Bei den Diagrammen könnte die Funktionalitäten von Plotly Express ausschöpfen. Auch könnten mehr Interaktionen stattfinden.
Die Dash\_core\_components bieten wie Slider für das Datum, Jahr \dots
Solche interaktiven Komponenten sind nicht schwierig zu implementieren, sorgen aber dafür, dass der Programmcode deutlich vergrößern könnte.
Dessen ungeachtet sollte ebenso der Programmcode refactored werden. Diese wäre notwendig in dem data\_prep-Modul, dass aus einer Datei mit der
Basis- und den Kindklassen besteht. In Python ist die Struktur mehrerer Klassen in einer Modul durchaus üblich. Viele Module der Python StandardBibliothek
haben viele Klassen in einer Datei. Diese sind dann einfacher zu importieren. Sollte aber die Codebase durch Hinzunahme weiterer
Daten weiter wachsen, spricht wegen der Übersichtlichkeit alles dafür diese Klassen in weiteren Dateien aufzusplitten. Auch der Programmcode an sich
könnte sicherlich vereinfacht werden. Hier muss man abwägen zwischen Lesbarkeit und kondensierter Komplexität\cite[Vgl.][]{ousterhout_philosophy_2018}.
Sicherlich kann auch der Programmcode optimiert werden. Dies betrifft auch die Implementation der Dashboard-Logik. Da die Diagramme in dem 
Dashboard ähnliche Parameter empfangen, ließen sich hier vermutlich Funktionen / Methoden für einen Diagrammty entwickeln. Ferner wäre
es schön, wenn die Layoutlogik (Hintergrundfarbe, Größe) von dem Erstellen der Diagramme separiert werden könnte.

Viel Zeit in der vorliegenden Arbeit wurde auf die Datenanalyse verwendet. Dagegen viel es relativ leicht die Datenvisualisierungen und das Dashboard
zu erstellen. Die Datenanylse der Daten aus vielen Bereichen war dann viel. Ebenso war es am Anfang schwierig mi Python und pandas.
Da es das erste große Projekt mit Python und den pandas, plotly und dash war, war der Beginn der Entwicklung schwierig.
Nichtsdestotrotz war es die richtige Wahl dieses Projekt mit den ausgewählten Sprache und den Bibliotheken umzusetzen, da sie sehr mächtig und
flexibel sind. Dies zeichnen sich unter anderem dadurch aus, dass es mehrere Lösungsalternativen für ein Problem geben kann. Gerade als
Anfänger da schon den Überblick verlieren kann. Während insbesondere der Programmierung gab es eine Lernkurve, die als bereichernd empfunden wurde.
Die Planung für ein Projekt mit abgesteckten Zeitrahmen ist sehr wichtig und hat bis auf ein paar Ausnahmen sehr gut funktioniert.
Mit dem Fortschreiten des Projektes hat man eine bessere Aufwandseinschätzung gewonnen, insbesondere in der Programmierung, aber auch im schreiben
des theoretischen Teils. So zum Beispiel bei manchen Fehlern, die frühzeitig gemacht wurden, und die erst spät auffallen. 
Das verlangt eine Flexibilität ab, aber auch einen Mut zur Lücke.

    % % \section{Welche Themen wurden nicht bearbeitet}
    % Zerklopfen der Module -> data\_prep falls noch mehr Methoden dazukommen  riesige Datei Auslagerung der Child classes in eigene Dateien
    % -> flat is always better
    % Wurde nicht alles im OO-Style umgesetz
    % Python Dateien besser aufteilen -> Data prep eine Datei mit bis zu knapp 1000 Zeilen Code
    %     Sollte Codebase noch wachsen -> in einzelne dateien - > habe ich aber trotzdem erstmal so gelassen
    %     keine eindeutige Regel -> sondern Konvention für und wieder, Viele Module der Python StandardBibliothek
    %     haben viele Klassen in einer Datei -> it's easier to Import

    % Reducing complexity 


% \section{Welche Themen sind im Anschluss denkbar}
% Cron-Job -> automatisches Starten des Skriptes
% Test der einzelnen Methoden
% Counter-Statistiken
% Es wurden als Datengrundlage weiterhin nur die Daten berücksichtigt, 
% die für die Anwendungsfälle benötigt werden. Außen vor blieb zum Beispiel die Integration der \acrshort{COP 5}-Statistiken.
% Zugriff auf elektronische Ressourcen Schon auch wichtiger Bereich innerhalb der MPG
% Fehlermeldungen loging des erfolgreichen
% \\
% Auswertung Zeitschriften, aus denen die Artikel sind -> Aufschluß darüber, ob es sich lohnt Zeitschriften anzuschaffen.
% Zusammenführen der Daten Ausleihe mit Bestand -> ein großes dataset
% \\
%bash script für den import der Dateien
%  RVK -> sprechende Benennungen einführen
%  Datenbank-Anbindung
% \\
% Refactoring -> wenn System produktiv gehen soll.
% Zerklopfen der Module -> data\_prep falls noch mehr Methoden dazukommen  riesige Datei Auslagerung der Child classes in eigene Dateien
% -> flat is always better
% Wurde nicht alles im OO-Style umgesetz
% Python Dateien besser aufteilen -> Data prep eine Datei mit bis zu knapp 1000 Zeilen Code
%     Sollte Codebase noch wachsen -> in einzelne dateien - > habe ich aber trotzdem erstmal so gelassen
%     keine eindeutige Regel -> sondern Konvention für und wieder, Viele Module der Python StandardBibliothek
%     haben viele Klassen in einer Datei -> it's easier to Import

% Reducing complexity \cite[Vgl.][]{ousterhout_philosophy_2018}
% \\
% -> sich darein denken erfordert nochmal ein bisschen mehr Zeit.
% Die Dateien zur Datenanreicherung müssen manuell gepflegt werden -> automatischer Prozess -> könnte noch ein bisschen vereinfacht werden,
% programmiertechnisch

% klarere Trennung zwischen den Teilsystemen, insbesondere die Datenanreicherung angeht

% Proportionen der Diagramme mehr beachten, um visuell besser unterscheiden zu können -> Plotly-Vorgaben, Überarbeitung des Layouts,
% weitere Möglichkeiten der statistischen Auswertung in Betracht ziehen. Korrelationen berechnen zwischen der Bestandsgröße und der Ausleihe
% Anzeigen von Entwicklungen durch Trendlinien 

% Das Dashboard-Layout überarbeiten. -> Responsive Design hinzufügen
% Anordnung der Diagramme sowie das Anzeigen der Cards verbessern.


% \section{Lessons learned}
Viel Zeit für Datenanalyse draufging
war viel Daten aus allen möglichen Bereichen -> hoher Aufwand
relativ einfach Datenvisualisierungen zu erzeugen mit den Frameworks
gute Zeitplanung alles ist
Viele Dinge zu berücksichtigen gilt -> Frontend Gestaltung des Dashboardes -> ausbaufähig
Python und pandas mächtig und flexibel ->  bieten  viele Möglichkeiten an, die es manchmal 
dann ein bisschen zu verwirrend macht. Viele neue Dinge gelernt, während des Programmierens
Lernkurve merkt man
Zeiteinteilung -> bessere Aufwandseinschätzung weiß wie aufwendig die Sachen, gerade wenn man festestellt,
dass ein Fehler behoben werden muss, und anwelchen
Dass Fehler, die man Anfang macht -> dass die manchmal später zurück kommen -> Teilsysteme nicht so richtig trennen


